---
title: "Basic OSS Architecture"
---

The purpose of this project is to showcase the power of open source tools when designing a data and analytics system. I will be walking through my workflow step by step, and including both images, code, and notes.

## Project Initialization
### First Steps
Let's start from scratch, a blank [VS Code](https://www.code.visualstudio.com/) IDE. We'll do everything from the command line, so make sure to open that up (Ctrl-`).

![Blank IDE](images/blank_vscode_ide.png)

First, I begin in my home directory. Then, I change to my Documents directory, which I use for all of my projects. This is where I'll begin creating the project directory and initializing the subsequent tools. As you'll see below, I first initialize the uv repository and change into it. Then, I create the repo on GitHub (because I like generating the license then), pull (my global is set to merge), commit, and make the initial push. Then, I will initialize the Quarto project, to begin documentation as I work. 

![Project and Repo Initialization](images/uv_gh_init.png)

![Basic Uv Environment](images/uv_base_env.png)
You can see here that I have both `jupyter` and `dbt` already installed. That's because `uv` installs tools system wide, because these are typically used from the CLI. That being said, some CLI tools (like Quarto and DuckDB) in my experience don't work with `uv` because it doesn't install their executables. 

![Git Remote Add and Pull](images/gh_remote_add.png)

![First Commit](images/first_commit.png)

#### Adding Quarto
Now, it's time to setup some extra functionality in the project. I'm going to be using Quarto for documentation, so I'll run `quarto create project`. To learn more about Quarto and configuring your documentation in projects, checkout my [guide](https://ChrisKornaros.github.io/guides/quarto/). It's a fantastic tool for building beautiful, robust documentation, even in enterprise production environments. Consider it for future papers, websites, dashboards, and reports. 

That being said, if you are ever taking screenshots of your work and want to quickly move them into your images folder, you can do so from the CLI.

![Quarto Project Initialization](images/quarto_init.png)

![Moving Images from the CLI](images/mv_cli_images.png)

Now that you've done that, it's time to start adding dependencies. As a heads up, don't be surprised if you don't see the `uv.lock` or the `.venv` objects in your directory right away, because `uv` doesn't create those until you add dependencies. Simply run `uv add` to start adding them. Afterwards, the necessary requirements and lock files will update automatically. If you want to learn more, checkout my [uv guide](https://ChrisKornaros.github.io/guides/uv).

![Add Dependencies with uv](images/uv_add.png)

#### The Pyproject.toml file
Once that's done, uv will update the general dependencies in the `pyproject.toml` file and the specific versions in `uv.lock` (think requirements.txt on steroids). The nice thing here, it only lists the actual package you needed, not everything else that the package requires. So, when you want to remove packages you can simply use `uv remove` and the individual package names listed here to remove *everything* in your environment. There's an example below. 

```python
[project]
name = "basic-oss-architecture"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "dbt-duckdb>=1.9.1",
    "dbt>=1.0.0.38.22",
    "duckdb>=1.1.3",
    "great-expectations>=0.18.22",
    "jupyter>=1.1.1",
    "pandas>=2.2.3",
    "pytest>=8.3.4",
    "quarto>=0.1.0",
]
```

#### Using .gitignore effectively
You probably noticed, but when you initialize a project with `uv` it automatically creates a `.gitignore` file and populates it with basic files and directories which don't need to be checked into source control (like .venv). I take this a step further, and add some Quarto specific files and directories too, `.quarto` and `_files` folders. Managing this file effectively can drastically reduce the file size of your commits.

Below is an example of my file at this early project stage.

![](images/gitignore_sample.png)

### Initializing a Data Environment
Now, you'll be setting up dbt. Similar to the other CLI tools, dbt uses the `dbt init` command to create the folder structure necessary for the program to be effective. As you can see below, the process is very easy. You'll only enter a name for your project, which will (case sensitively) become the name of the dbt directory. Next, we'll walk through the fundamental pieces of a dbt project in depth. 

![](images/dbt_init.png)

### The Data Build Tool (dbt)
As I said, creating a dbt project is easy, but it can get confusing from here on out if you're alone with the [dbt documentation](https://docs.getdbt.com/guides/manual-install?step=1). In my experience, dbt initalizes a `logs/` folder in the project root directory **not** the dbt root directory. So, I make sure to add that to the gitignore file, because I don't think that needs to be checked into version control.

So, now that you've initialized your folder, let's go through the basics: 

#### Project Root: Basic_OSS
In the case of my project, the root folder is called `Basic_OSS`. Here, you'll find the 6 subdirectories, a `.gitignore` file, a `README.md` file, and the `dbt_project.yml` file. The `.gitignore` can be deleted, because you have one in the project root directory, and for the same reason, so can the README. The dbt project file is the core of your entire data environment, in the same exact way that a `_quarto.yml` file is the core of your website, book, or documentation project. 

This is where you'll configure the actual structure and hierarchy of your environment, along with things like schemas or variables, or aliases. 

#### Analyses
Contains the SQL files for any analysis done that are not part of the core models. Think of these as the `SELECT` statements for analytical queries, whereas models handle the `DDL` statements for database architects. Depending on your workflow, this folder could be unused.

#### Macros
This is where you can store custom macros (functions) and reusable code written in either SQL or `jinja`. This is the Python package equivalent for SQL and it's often used to ensure **DRY** (Don't Repeat Yourself) principles for ETL and other database work.

#### Models
This is the core of dbt. Models are the SQL tables themselves, as well as the transformations when cleaning and aggregating data (from raw to reporting). If you have a raw schema (where the raw data is temporarily stored) and a clean schema (where cleaned data is persisted), you would have both a `raw` and `clean` folder within the `models` folder. Then, the individual queries would live within those subfolders as the actual tables and views. 

It is where most of (if not all) your transformations live. So, can become computationally taxing if you aren't careful. 

Run with `dbt run` or `dbt run --select {model_directory_name}`.

#### Seeds
These are flat files containing static data used for mapping (or reference) data. Only use this if your project needs static data. For more on [seeds](https://docs.getdbt.com/reference/seed-configs).

Run with `dbt seed`.

#### Snapshots
Stores snapshot definitions for versioning and tracking changes in source data over time. These are commonly used for **SCDs** (slowly changing dimensions) or auditing.

Run with `dbt snapshot`.

#### Tests
Fairly self explanatory, but this folder contains custom, SQL-defined tests for your models. Dbt allows for both custom tests defined in `.sql` files and *generic* tests defined in a `YAML`. The tests run on various models are defined in the dbt_project.yml file. 

#### Extra Notes
Dbt also has the `docs/` and `dbt_packages/` folders which are for advanced documentation and shareable, modularized code, respectively. Generally speaking, your workflow will really only involve the following parts of a dbt project:

1. `models/`
2. `tests/`
3. `macros/`
4. `dbt_project.yml`

The others are optional and provide functionality, that while useful and powerful in many cases, is not always needed. Now that we've got the local directory all configured, it's time to start building the container for our PostgreSQL instance (server, cluster, whatever you want to call it).

### Docker and Containers